{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkLNbAF5Cprj"
      },
      "outputs": [],
      "source": [
        "# Some Required installation\n",
        "\n",
        "!pip install kaggle\n",
        "!pip install  keras_core keras_nlp\n",
        "!pip install Keras-Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from keras.utils import pad_sequences\n",
        "from keras.utils import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense,Dropout,Input,Embedding,Flatten,TextVectorization,Conv1D,GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D\n",
        "from keras.initializers import Constant\n",
        "from keras.layers import Dense,LSTM,Bidirectional,Attention,Concatenate,GRU,BatchNormalization\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import seaborn as sns\n",
        "nltk.download('stopwords')\n",
        "import keras_core as keras\n",
        "import keras_nlp\n",
        "\n"
      ],
      "metadata": {
        "id": "gKsqnOtkCvKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable multiple gpus\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if len(gpus)<=1:\n",
        "    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
        "    print(f'Using {len(gpus)} GPU')\n",
        "else:\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "    print(f'Using {len(gpus)} GPUs')"
      ],
      "metadata": {
        "id": "LPyNGXUIC2cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/content/SC_4label.csv')\n",
        "data"
      ],
      "metadata": {
        "id": "7CowEZjJC5KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=['DD','IO','RE','TD']\n",
        "plt.figure(figsize = (8,5))\n",
        "ax = sns.countplot(x = data['label_encoded'], palette = 'Set1', alpha = 0.8)\n",
        "ax.set_xticklabels(labels)  # Set custom x-axis labels\n",
        "#plt.title('Distribution of vulnerabilities')\n",
        "plt.savefig('vul_distribution.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZGDyq2ELC8Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data['code']\n",
        "y = data['label_encoded']"
      ],
      "metadata": {
        "id": "N9psStsRC_TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solidity_stopwords = [\n",
        "    \"pragma\", \"interface\", \"contract\", \"function\", \"event\", \"modifier\", \"library\", \"using\",\n",
        "    \"string\", \"uint8\", \"uint256\", \"address\", \"mapping\", \"bool\", \"require\", \"return\", \"memory\",\n",
        "    \"storage\", \"public\", \"internal\", \"view\", \"returns\", \"constant\", \"constructor\",\n",
        "    \"_owner\", \"_balances\", \"_allowances\", \"_founder\", \"_marketing\", \"_who\", \"_burntAmount\",\n",
        "    \"_from\", \"_to\", \"_value\", \"_timestamp\", \"_bool\", \"msg.sender\", \"totalSupply\",\n",
        "    \"balanceOf\", \"transfer\", \"allowance\", \"approve\", \"transferFrom\", \"add\", \"sub\", \"mul\", \"div\",\n",
        "    \"mod\", \"changeFounder\", \"setMinter\", \"setFurnace\", \"freezeAccount\",\"solidity\",\"bytes32\"\n",
        "]\n",
        "def clean_solidity_code(solidity_code):\n",
        "    # Remove comments (both single-line and multi-line)\n",
        "    cleaned_code = re.sub(r'//.*?$', '', solidity_code, flags=re.MULTILINE)\n",
        "    cleaned_code = re.sub(r'/\\*.*?\\*/', '', cleaned_code, flags=re.DOTALL)\n",
        "\n",
        "    # Remove special characters and punctuation\n",
        "    cleaned_code = re.sub(r'[^a-zA-Z0-9\\s]', '', cleaned_code)\n",
        "\n",
        "    # Remove extra whitespace and blank lines, and convert to lowercase\n",
        "    cleaned_code = '\\n'.join(line.strip().lower() for line in cleaned_code.splitlines() if line.strip())\n",
        "    # Remove common English stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in cleaned_code.split() if word not in stop_words]\n",
        "    tokens = [token for token in tokens if token not in solidity_stopwords]\n",
        "    cleaned_code = ' '.join(tokens)\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "qmFlhONnDCOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_cleaned = X.apply(clean_solidity_code)\n",
        "X_cleaned_sentences = [' '.join(doc) for doc in X_cleaned]\n",
        "X_cleaned_sentences=np.array(X_cleaned_sentences)\n",
        "X_cleaned_sentences[1]"
      ],
      "metadata": {
        "id": "BWiovtNoDE8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_cleaned_sentences, y,\n",
        "                                                    test_size=0.1,shuffle=True,random_state=42,stratify=y)"
      ],
      "metadata": {
        "id": "kYOeBmE3DP_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encoding label\n",
        "y_train_encoded = to_categorical(y_train, 4)\n",
        "y_test_encoded = to_categorical(y_test, 4)\n",
        "y_train_encoded"
      ],
      "metadata": {
        "id": "QEldtPiaDb9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Define the vocabulary size and sequence length\n",
        "vocab_size = 10000  # adjust as needed\n",
        "sequence_length = 100  # adjust as needed\n",
        "\n",
        "# Initialize TextVectorization layer\n",
        "vectorizer = TextVectorization(max_tokens=vocab_size, output_sequence_length=sequence_length)\n",
        "\n",
        "# Adapt the vectorizer on the training data\n",
        "vectorizer.adapt(X_train)\n",
        "\n",
        "# Vectorize the training and testing data\n",
        "X_train_sequences = vectorizer(X_train)\n",
        "X_test_sequences = vectorizer(X_test)\n"
      ],
      "metadata": {
        "id": "ibt0BasZDgGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding the sequences\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=128, padding='post', truncating='post')\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=128, padding='post', truncating='post')\n",
        "print('Shape of training tensor: ', X_train_padded.shape)\n",
        "print('Shape of testing tensor: ', X_test_padded.shape)"
      ],
      "metadata": {
        "id": "KOr5XhoKDiPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Smote oversampling\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "import math\n",
        "\n",
        "def smote(x, y):\n",
        "    k_neighbors = math.ceil(sum(y) * 0.01)\n",
        "\n",
        "    smote = SMOTE(sampling_strategy=1,\n",
        "                  k_neighbors=k_neighbors)\n",
        "    x, y = smote.fit_resample(x, y)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def bordersmote(x, y):\n",
        "    k_neighbors = math.ceil(sum(y) * 0.01)\n",
        "    m_neighbors = math.ceil(sum(y) * 0.01)\n",
        "\n",
        "    bordersmote = BorderlineSMOTE(sampling_strategy=1,\n",
        "                                  k_neighbors=k_neighbors,\n",
        "                                  m_neighbors=m_neighbors)\n",
        "\n",
        "    x, y = bordersmote.fit_resample(x, y)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "smote = SMOTE(sampling_strategy='minority', random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_padded, y_train_encoded)"
      ],
      "metadata": {
        "id": "BNaYn880Dk3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_resampled\n",
        "label_counts = np.sum(y_train_resampled, axis=0)\n",
        "max_length = max([len(w) for w in X_train])\n",
        "print(max_length)\n",
        "emb_len = len(vectorizer.get_vocabulary())\n",
        "print(emb_len)"
      ],
      "metadata": {
        "id": "-qlJmxl0Dllz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preset= \"albert_base_en_uncased\"\n",
        "\n",
        "preprocessor = keras_nlp.models.AlbertPreprocessor.from_preset(preset, sequence_length=128)\n",
        "classifier = keras_nlp.models.AlbertClassifier.from_preset(preset,preprocessor=preprocessor,num_classes=4)\n",
        "classifier.summary()\n"
      ],
      "metadata": {
        "id": "A0cyjqTyHBLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = '/content/saved_model/best_model_albert.keras'  # Use .keras extension\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,  # Save the entire model, not just weights\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "from keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
        "early_stop = EarlyStopping(monitor='val_loss',patience=3,verbose=True,restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n"
      ],
      "metadata": {
        "id": "OpGCtiIQHEiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile\n",
        "classifier.compile(\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "    metrics= [\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "vRbKVTqtHHJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit\n",
        "history = classifier.fit(x=X_train,\n",
        "                         y=y_train_encoded,\n",
        "                         batch_size=16,\n",
        "                         epochs=20,\n",
        "                         validation_data=(X_test,y_test_encoded),)"
      ],
      "metadata": {
        "id": "LSSn84BWHJzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.save(\"/content/saved_model/final_model_albert.keras\")"
      ],
      "metadata": {
        "id": "apcPvh2xHM5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy=classifier.evaluate(X_test,y_test_encoded)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "7G8MylbwHN_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Get the actual number of epochs the model trained for\n",
        "epochs_range = range(len(acc))  # Use the length of 'acc' instead of a fixed range\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.savefig('train_val_acc_albert.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QJcBg967HSle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(history, metric):\n",
        "    train_metric = history.history[metric]\n",
        "    val_metric = history.history[f'val_{metric}']\n",
        "    epochs = range(1, len(train_metric)+1)\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.plot(epochs,train_metric,label = f'Training {metric}')\n",
        "    plt.plot(epochs,val_metric,label = f'Validation {metric}')\n",
        "    plt.title(f'Training and Validation {metric.capitalize()}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.legend(['train', 'test'],loc='upper left')\n",
        "    plt.savefig('train_val_loss_albert.pdf')\n",
        "    plt.show()\n",
        "#Training and validation loss plot\n",
        "plot_metrics(history, metric='loss')"
      ],
      "metadata": {
        "id": "ZaJuqt98HTlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred_class = y_pred.argmax(axis=1)"
      ],
      "metadata": {
        "id": "sdEBffXiHYjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=['DD','IO','RE','TD']"
      ],
      "metadata": {
        "id": "rnsauzQmHZXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(y_test, y_pred_class,target_names=labels)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "-sUigemPHdiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "cm=confusion_matrix(y_test,y_pred_class)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=labels)\n",
        "disp.plot()\n",
        "plt.savefig('confmat_albert.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_XjpAtImHgSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unlabeled_data = pd.read_csv('/content/SC_unlabeled.csv')\n",
        "X_unlabeled = unlabeled_data['code']\n",
        "X_unlabeled"
      ],
      "metadata": {
        "id": "eoBaF-mjHi3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_cleaned_unlabeled = X_unlabeled.apply(clean_solidity_code)\n",
        "X_cleaned_unlabeled_sentences = [' '.join(doc) for doc in X_cleaned_unlabeled]\n",
        "X_unlabeled_sequences = vectorizer(X_cleaned_unlabeled_sentences)\n",
        "X_unlabeled_padded = pad_sequences(X_unlabeled_sequences, maxlen=128, padding='post', truncating='post')\n"
      ],
      "metadata": {
        "id": "Qz_kErHoHlXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preset= \"albert_base_en_uncased\"\n",
        "preprocessor = keras_nlp.models.AlbertPreprocessor.from_preset(preset, sequence_length=128)\n",
        "classifier = keras_nlp.models.AlbertClassifier.from_preset(preset,preprocessor=preprocessor,num_classes=4)\n",
        "classifier.load_weights('/content/saved_model/final_model_albert.keras')\n"
      ],
      "metadata": {
        "id": "qoFmVqqcHn1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_unlabeled = classifier.predict(X_cleaned_unlabeled_sentences)\n",
        "\n",
        "# Get the predicted class for each input\n",
        "predicted_classes = y_pred_unlabeled.argmax(axis=1)\n",
        "print(predicted_classes)\n",
        "\n",
        "#unlabeled_data['predicted_class'] = predicted_classes\n",
        "#unlabeled_data.to_csv('predicted_unlabeled_data.csv', index=False)\n",
        "\n",
        "y_actual = data['label_encoded']  # Replace 'label_encoded' with the actual label column name\n",
        "#y_actual\n",
        "\n",
        "# Evaluate the predictions\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_actual, predicted_classes))\n",
        "\n",
        "# Display the confusion matrix\n",
        "cm = confusion_matrix(y_actual, predicted_classes)\n",
        "labels = ['DD', 'IO', 'RE', 'TD']  # Replace with your class names\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(cmap='viridis')\n",
        "plt.savefig('confmat_albert_unlabeled.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VFA01WarHrdl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}